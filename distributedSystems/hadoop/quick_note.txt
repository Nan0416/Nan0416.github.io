// HDFS manages data on a cluster
// YARN manages computational resource on a cluster
// To run a toy mapreduce program, the two components are optional
// but they are necessary for a large program.
namenode 
web-protal(Web Interface): 50070
service-protal:9000

datanode 
web-protal: 50075
service-protal:50010

start hdfs
1. /etc/hadoop/core-site.xml config hdfs address
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
2. hdfs attributes, #replication
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

3. 
bin/hdfs namenode -format #initalize formatting
sbin/start-dfs.sh #start namenode, secondary namenode and datanodes
sbin/stop-dfs.sh #stop namenode, secondary namenode and datanodes

start YARN
ResourceManager
web-protal: 8088
service: 8032

NodeManager


sbin/start-yarn.sh
sbin/stop-yarn.sh


physical storage location
/tmp/hadoop-nan/dfs/data/current


///// cluster /////
Typical configuration.
one machine (master): namenode
one machine (master): resource manager

other machines run both datanode and nodemanager(worker: responsible for launching and managing containers on a node. Containers execute tasks as specified by the AppMaster)


Example of 2 machines HDFS cluster
Two linux machines
1. 192.168.0.102 linux-e435 (datanode)
2. 192.168.0.110 linux (namenode, datanode)

close firewall
** sudo ufw disable
/////////// 
1. configurating two machines' hosts file.
/etc/hosts add the above two entries. (make sure no duplicated name)


2. configure /etc/hadoop/slaves
because both of machines are datanodes, we add both hostnames here
linux-e435
linux

3. configure /etc/hadoop/core-site.xml
select linux as the namenode
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://linux:9000</value>
</property>

4. configure /etc/hadoop/hdfs-site.xml. Add namenode and datanode directories and replication number.

   <property>
            <name>dfs.namenode.name.dir</name>
	    <value>/home/nan/Documents/hadoop/project/cluster/namenode</value>
    </property>

    <property>
            <name>dfs.datanode.data.dir</name>
	    <value>/home/nan/Documents/hadoop/project/cluster/datanode</value>
    </property>	
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>

** the cluster has to be created manually
** the replication number <= actual #data nodes.

5. format on the namenode
hdfs namenode -format.
***
when update the slave number or node configuration, such as adding new nodes.
you have to reformat hdfs. But before reformatting, manually delete the namenode and datanode directory.



