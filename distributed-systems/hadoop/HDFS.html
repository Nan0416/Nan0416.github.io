<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- NewPage -->
<html lang="en">
<head>
<title>Hadoop&nbsp;HDFS</title>
<meta charset="utf-8">
<meta name="date" content="2019-02-03">
<meta name="keywords" content="">
<meta name="keywords" content="">
<meta name="keywords" content="">
<link rel="stylesheet" type="text/css" href="../../stylesheet.css" title="Style">
<script type="text/javascript" src="../../script.js"></script>
<script type="text/javascript" src="../../syntaxHighlight_js_c/XRegExp.js"></script>
<script type="text/javascript" src="../../syntaxHighlight_js_c/shCore.js"></script>
<script type="text/javascript" src="../../syntaxHighlight_js_c/shBrushCpp.js"></script>
<script type="text/javascript" src="../../syntaxHighlight_js_c/shBrushJava.js"></script>
<script type="text/javascript" src="../../syntaxHighlight_js_c/shBrushJScript.js"></script>
<script type="text/javascript" src="../../syntaxHighlight_js_c/shBrushBash.js"></script>
<link href="../../syntaxHighlight_css_c/shCore.css" rel="stylesheet" type="text/css" />
<link href="../../syntaxHighlight_css_c/shThemeDefault.css" rel="stylesheet" type="text/css" />

<script src="https://d3js.org/d3.v4.min.js"></script>

</head>
<body>
<script>
SyntaxHighlighter.config.strings.expandSource = '+ expand source';
SyntaxHighlighter.config.strings.help = '?';
SyntaxHighlighter.config.strings.alert = 'SyntaxHighlighter\n\n';
SyntaxHighlighter.config.strings.noBrush = 'Can\'t find brush for: ';
SyntaxHighlighter.config.strings.brushNotHtmlScript = 'Brush wasn\'t configured for html-script option: ';
SyntaxHighlighter.defaults['pad-line-numbers'] = false;
SyntaxHighlighter.defaults['toolbar'] = false;
SyntaxHighlighter.all()
</script>
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="bar">
<strong>Hadoop&nbsp;-&nbsp;HDFS</strong>
</div>
<p class="date"><span class="created-date">Created:2019-02-11</span>&nbsp;&nbsp;<span class="last-modified">Last modified:2019-02-18</span></p>
<div class="catalog">
<ul class="catalogItems">
<li><a href="#intro">Introduction</a></li>
<li><a href="#arch">Architecture</a></li>
<li><a href="#usage">How to use</a></li>
<li><a href="#reference">References</a></li>
</ul>
</div>
<hr>
<div class="contentContainer">
<ol>
<li>
<div class="content" id="intro">
<h3>Introduction</h3>
<p>HDFS (Hadoop Distributed FS)</p>
<h4>Design Principle</h4>
<div class="featureList">
    <ol>
        <li>HDFS implements POSIX APIs with some exception. HDFS is good at batch processing, which means it focus on high throughtput instead of low latency. (A typical file in HDFS is gigabytes to terabytes in size)</li>
        <li>Write once read many: Modification on a file only allows to append or truncate at the end of file instead of arbitrary points.</li>
        <li>No hardlink and softlink concept</li>
    </ol>
</div>

</div>
</li>
<li>
    <div class="content" id="arch">
        <h3>Architecture</h3>
        <div class="featureList">
            <ol>
                <li>client program: break a file into blocks (64MB or 128MB), calculate checksum, asking NameNode about a few DataNode for each block</li>
                <li><pre>
NameNode: (single point of failure) manage DataNodes, handle “under replicated” case, provide APIs, such as opening, closing, and renaming files and directories, 
and store file’s metadata, such as
    filename -> [
        block 1-> [DN1, DN3, DN6],
        block 2 ->[DN2, DN4, DN5],
        block 3 ->[DN1, DN2, DN5]]
    datanode->[
        DN1 -> [block 1, block 3]
            …
        ]
    *** how to select datanodes?
    1). Each datanode only has one replica
    2). At most two selected datanode on the same rack.
[DataNodes are organized in racks. Each rack has multiple datanodes]
                    </pre>
                </li>
                <li>
                    DataNode: store data along checksum, help forward data, send heartbeat to NameNode, periodically sent BLOCKReport to name server.
                </li>
            </ol>
        </div>
    </div>
</li>
<li>
    <div class="content" id="usage">
        <h3>How to use (Hadoop 2.9.2)</h3>
        <div class="featureList">
            <h4>Starting a HDFS cluster</h4>
            <p>Two Linux machines: One Linux (linux: 192.168.0.110) runs a NameNode and a DataNode, another Linux (linux-e435:192.168.0.102) runs a DataNode</p>
            <ol>
                <li>
1). sudo ufw disable # disable firewall
2). configure /etc/hosts to correct addresses, make sure no duplicated hostnames
                </li>
                <li>Configure /etc/hadoop/core-site.xml to tell using HDFS and its address
<pre style="border:1px green solid">
    &lt;configuration>
        &lt;property>
            &lt;name>fs.defaultFS&lt;/name>
            &lt;value>hdfs://localhost:9000&lt;/value>
        &lt;/property>
    &lt;/configuration>
</pre>
                </li>
                <li>Configure /etc/hadoop/hdfs-site.xml, tell where is data physicall stored and #replication ( the replication number &lt;= actual #data nodes)<br>
                    We also need to create the directory /home/nan/Documents/hadoop/project/cluster
<pre style="border:1px green solid">
    &lt;property>
        &lt;name>dfs.namenode.name.dir&lt;/name>
	    &lt;value>/home/nan/Documents/hadoop/project/cluster/namenode&lt;/value>
    &lt;/property>
    &lt;property>
        &lt;name>dfs.datanode.data.dir&lt;/name>
	    &lt;value>/home/nan/Documents/hadoop/project/cluster/datanode&lt;/value>
    &lt;/property>	
	&lt;property>
        &lt;name>dfs.replication&lt;/name>
		&lt;value>1&lt;/value>
    &lt;/property>
</pre>
                </li>
                <li>Configure workers' addresses in /etc/hadoop/slaves
<pre style="border:1px green solid">
    linux-e435
    linux
</pre>
                </li>
                <li>
<pre class="brush:bash">
$hdfs namenode -format #initalize formatting
$start-dfs.sh #start namenode, secondary namenode and datanodes
$stop-dfs.sh #stop namenode, secondary namenode and datanodes
</pre>
                </li>
            </ol>
        </div>
        <div class="featureList">
            <h4>Use a HDFS</h4>
            <ol>
                <li>Web interface: namenode @ IP:50070, datanode @ IP:50075</li>
                <li>API programming: namenode @ IP:9000, datanode @ IP:50010</li>
                <li>CLI
                    <pre class="brush:bash">
hdfs dfs -df -h # check storage capacity
hdfs dfs -ls / # list contents of the hdfs's root directory
hdfs dfs -mkdir /data # make dir
hdfs dfs -put /local/file.txt /data # copy data from local fs to hdfs
hdfs dfs -get /data/file.txt /local # copy data from hdfs to local fs
hdfs dfs -cat /data/file.txt # print file.txt in hdfs
                    </pre>
                </li>
            </ol>
        </div>
        <div class="featureList">
            <h4>Common Error and solution</h4>
            <ol>
                <li>Remote DataNodes do not show up?<br>
Issue 1: Version. Manually delete the namenode and datanode directory. And reformat.<br>
Issue 2: Firewall. sudo ufw disable #disable firewall<br>
Isuue 3: Duplicated entries in /etc/hosts 
                </li>
                <li>Reconfigure #datanodes?<br>
Have to delete two directories and reformat.
                </li>
            </ol>
        </div>
        
    </div>
</li>
<li>
<div class="content" id="reference">
<h3>References</h3>
<div class="featureList">
    <ol>
        <li><a href="./files/HDFS.pdf" target="_blank">HDFS architecture</a></li>
        <li><a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Introduction" target="_blank">HDFS Apache</a></li>
    </ol>
</div>
</div>
</li>
</ol>
</div>
    
</body>
</html>
