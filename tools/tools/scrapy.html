<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- NewPage -->
<html lang="en">
<head>
<title>Tools&nbsp;Scrapy</title>
<meta charset="utf-8">
<meta name="date" content="2019-10-30">
<meta name="keywords" content="spider">
<meta name="keywords" content="python">
<meta name="keywords" content="scrapy">
<link rel="stylesheet" type="text/css" href="../../stylesheet.css" title="Style">
<script type="text/javascript" src="../../script.js"></script>
<script type="text/javascript" src="../../syntaxHighlight_js_c/XRegExp.js"></script>
<script type="text/javascript" src="../../syntaxHighlight_js_c/shCore.js"></script>
<script type="text/javascript" src="../../syntaxHighlight_js_c/shBrushPython.js"></script>
<script type="text/javascript" src="../../syntaxHighlight_js_c/shBrushCpp.js"></script>
<script type="text/javascript" src="../../syntaxHighlight_js_c/shBrushJava.js"></script>
<script type="text/javascript" src="../../syntaxHighlight_js_c/shBrushJScript.js"></script>
<script type="text/javascript" src="../../syntaxHighlight_js_c/shBrushBash.js"></script>
<script type="text/javascript" src="../../syntaxHighlight_js_c/shBrushSql.js"></script>
<link href="../../syntaxHighlight_css_c/shCore.css" rel="stylesheet" type="text/css" />
<link href="../../syntaxHighlight_css_c/shThemeDefault.css" rel="stylesheet" type="text/css" />

<script src="https://d3js.org/d3.v4.min.js"></script>

</head>
<body>
<script>
SyntaxHighlighter.config.strings.expandSource = '+ expand source';
SyntaxHighlighter.config.strings.help = '?';
SyntaxHighlighter.config.strings.alert = 'SyntaxHighlighter\n\n';
SyntaxHighlighter.config.strings.noBrush = 'Can\'t find brush for: ';
SyntaxHighlighter.config.strings.brushNotHtmlScript = 'Brush wasn\'t configured for html-script option: ';
SyntaxHighlighter.defaults['pad-line-numbers'] = false;
SyntaxHighlighter.defaults['toolbar'] = false;
SyntaxHighlighter.all()
</script>
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="bar">
<strong>Tools&nbsp;-&nbsp;Scrapy</strong>
</div>
<p class="date"><span class="created-date">Created:2019-10-30</span>&nbsp;&nbsp;<span class="last-modified">Last modified:2019-11-26</span></p>
<div class="catalog">
<ul class="catalogItems">
<li><a href="#scrapy">Scrapy</a></li>
<li><a href="#example">Example</a></li>
<li><a href="#scrapyd">Scrapyd</a></li>
<li><a href="#reference">References</a></li>
</ul>
</div>
<hr>
<div class="contentContainer">
<ol>
<li>
<div class="content" id="scrapy">
<h3>Scrapy (based on v1.8)</h3>
<p>Scrapy is python crawling framework. Developers need to define how to crawl a site, following links, extract data, persist data. The framework helps schedule crawling, download pages, and form the pipeline.</p>
<div class="featureList">
    <ol>
        <li>
            <h4>Architecture/WorkFlow</h4>
            <p>Scrapy allows to define multiple spiders in a project. Each spider is a job implemented as a subclass of <span class="inline-code">scrapy.Spider</span></p>
            <p>The spider class has the start urls, and yield <span class="inline-code">Request</span> to 'engine', the 'engine' send request to 'scheduler'</p>
            <pre class="brush:bash">
                spider -> (Request) -> scheduler -> downloader -> (Response) -> spider defined parser -> (Request) -> scheduler -> ...
                                                                                                      -> (Item) -> pipeline
            </pre>
            <p style="color:red">** The above process omit engine. **</p>
        </li>
        <li>
            <h4>request</h4>
            <p>Each request is a http request and the handler function. The handler function can yield another request, or generate an item.</p>
            <p>New requests are sent to scheduler, and new items are sent to item pipeline.</p>
            <pre class="brush:python">
                def start_requests(self):
                    for id in range(1, self.product_pages + 1):
                        yield scrapy.Request(
                            url = self.base_url % id,
                            callback = self.parse_metalink,
                            method = 'GET',
                            headers = self.headers
                        )
            </pre>
            <p>The start_requests is a framework defined function that will be invoked first. And the callback function is user designated function that can continuously yield requests or return items.</p>
        </li>
        <li>
            <h4>Item and pipeline</h4>
            <p>Item is a user-defined data structure, it's used to represent a result. The result will be sent to a pipeline of handler, such as persister.</p>
            <pre class="brush:python">
# example of item.
class Product(scrapy.Item):
    url = scrapy.Field()
    id = scrapy.Field()
    category = scrapy.Field()
    name = scrapy.Field()
    image_urls = scrapy.Field()
    price = scrapy.Field()
    review_num = scrapy.Field()
    brand = scrapy.Field()
    rating = scrapy.Field()
    department = scrapy.Field()

class FailedItem(scrapy.Item):
    url = scrapy.Field()
    reason = scrapy.Field()
            </pre>
            <p>Pipeline's handler are written as a class, and make a chain by the settings.py file, <span style="color:red">it must return the item.</span></p>
            <pre class="brush:python">
                    class SaveProductToFile(object):
                        file = None
                        def open_spider(self, spider):
                            self.file = open('./walmart_products/%s.json' % spider.filename, 'wb')
                            self.exporter = JsonItemExporter(self.file, indent=4)
                            self.exporter.start_exporting()
                    
                        def close_spider(self, spider):
                            self.exporter.finish_exporting()
                            self.file.close()
                    
                        def process_item(self, item, spider):
                            self.exporter.export_item(item)
                            return item
                    
            </pre>
            <p><span style="color:red">when accessing attributes from the item, using item['attr'] instead of item.attr</span> no javascript.</p>
        </li>
        <li>
            <h4>settings.py</h4>
            <p>settings.py contains framework specified settings, such as pipeline, logging level, and user sepecified settings, such as db-connection string.</p>
            <pre class="brush:python">
                    #settings.py
                    ITEM_PIPELINES = {
                        # 'scrapying_scripts.pipelines.SaveProductToFile': 300,
                        'scrapying_scripts.pipelines.SaveProductToMongoDB': 400,
                        'scrapying_scripts.pipelines.SaveFailedProductToMongoDB': 500,
                    }
                    DB = {
                        'MONGO_DB_URI': 'mongodb://localhost:27017',
                        'MONGO_DB_NAME': 'my_project'
                    }
            </pre>

        </li>
        <li>
            <h4>Installation/Template</h4>
            <p>Using a isolated python environment with virtualenv</p>
            <pre class="brush:bash">
                pip install scrapy # install the scrapy python package, also a command line
                pip install scrapyd # install the scrapy deployment server.
            
            </pre>
            <p>scrapy cmd can provide a starting template</p>
            <pre class="brush:bash">
                scrapy startproject [project-name] # create project template
                cd project; # cd into the project directory.
                scrapy crawl [spider-name] # run the specific spider by given name.
            </pre>
            <p>It will generate a directory named as the project name and it includes a python package and a .cfg configuration file.</p>
            <p style="color:red">Under the project directory, scrapy has more sub-commands.</p>
            <pre class="brush:bash">
                scrapy.cfg # define the metadata about the project, will be used when deploying the project.
                project_name/
                    items.py # a list of classes of Item
                    pipelines.py # a list of classes of item handler.
                    middlewares.py
                    settings.py 
                    spiders/ # a list of spider.py file, each spider can be executed independently.
                        spider1.py
                        spider2.py 
                        ...
            </pre>
        </li>
    </ol>
</div>
</div>
</li>   
<li>
<div class="content" id="example">
    <h3>Example</h3>
    <div class="featureList">
        <ol>
            <li>
                <h4>scrapy.spiders.Spider</h4>
                <p>Docs: https://docs.scrapy.org/en/latest/topics/spiders.html, Source: https://github.com/scrapy/scrapy/blob/1.8/scrapy/spiders/__init__.py</p>
                <p>A user defined spider has two way to provides start urls, <br>
                    1). define an attributes called <span class="inline-code">start_urls</span> of type list<br>
                    2). define a function <span class="inline-code">start_requests</span> yield Request.
                </p>
                <p>The difference between two ways is the function can yeild a request, which can specify the parse callback function, otherwise, using the default function called <span class="inline-code">parse(self, response)</span></p>
                <h4>Request class</h4>
                <p>Request class specifies: <br>
                    url, method, header, body, cookie, encoding // downloader required info<br>
                    priority, dont_filter // scheduler required info <span style="color:red">dont_filter = False (default) will remove duplicate urls to avoid loop</span>
                    callback, errback, meta, cb_kwargs // framework required info
                </p>
                <p>meta: meta is used to populate data through the pipeline, but it can also contain special keys that can be recoginized by the scrapy framework.</p>
                <h4>parse function</h4>
                <p>A spider class has a parser function that will be invoked after the downloader downloads the request.</p>
                <pre class="brush:python">
                        def parse(self, response, *args, **cb_kwargs):
                            # cb_kwargs comes from request.
                            pass
                        def parse(self, response):
                            pass
                </pre>
            </li>
        </ol>
    </div>
</div>
</li>
<li>
<div class="content" id="reference">
<h3>References</h3>
<div class="featureList">
    <ol>
        <li><a href="https://docs.scrapy.org/en/latest/topics/architecture.html" target="_blank">Scrapy Architecture</a></li>
    </ol>
</div>
</div>
</li>
</ol>
</div>
    
</body>
</html>
